import pandas as pd
import streamlit as st
from datetime import datetime


path = '../src'

now = datetime.now()
now_name = now.strftime('%Y%m%d')


## page setting
st.set_page_config(
    page_title='ì±„ìš©ê³µê³  í™•ì¸'
)
st.title('ì±„ìš©ê³µê³  í™•ì¸')
st.write('')

<<<<<<< HEAD
def make_dict():
    data_wanted = {'Lin': [], 'Tit': [], 'Com': [], 'Loc': [], 'Ctn': []}
    data_saramin = {'Lin': [], 'Tit': [], 'Com': [], 'Loc': [], 'Ctn': []}
    data_major = {'Lin': [], 'Tit': [], 'Com': [], 'Loc': [], 'Ctn': []}

    return data_wanted, data_saramin, data_major

## ìŠ¤í¬ë¡¤ ëê¹Œì§€
def scroll(driver):
    scroll_location = driver.execute_script('return document.body.scrollHeight')
    while True:
        driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')
        time.sleep(2)
        scroll_height = driver.execute_script('return document.body.scrollHeight')
        if scroll_location == scroll_height:
            break
        else:
            scroll_location = driver.execute_script('return document.body.scrollHeight')
    driver.implicitly_wait(3)


## ìŠ¤í¬ë¡¤ í•œ ë²ˆë§Œ
def scroll_one(driver):
    driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')
    time.sleep(1)
    driver.implicitly_wait(3)


## ìˆ˜ì§‘ëª…, ìˆ˜ì§‘ ê°œìˆ˜, ì²«ë²ˆì§¸ ê°’ ë°˜í™˜
def elem_return_0(Name, List):
    return print(f'\n{Name} *** {len(List)}\n{List[0]}')


## ìˆ˜ì§‘ëª…, ìˆ˜ì§‘ ê°œìˆ˜ ë°˜í™˜
def elem_return_1(Name, List):
    return print(f'\n{Name} *** {len(List)}')

## ì›í‹°ë“œ ìˆ˜ì§‘
def Wanted(KEYWORD, data_wanted):

    # webdriver ì‹¤í–‰
    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))
    driver.get(f'https://www.wanted.co.kr/search?query={KEYWORD}&tab=position')
    driver.implicitly_wait(2)
    scroll(driver)

    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶œë ¥
    print(f'\n\n##### {KEYWORD} #####')
    
    # ê³µê³  ê°œìˆ˜
    num = driver.find_element(By.XPATH, '//*[@id="search_tabpanel_position"]/div/div[1]/h2').text
    num = int(num.replace('í¬ì§€ì…˜', ''))
    for i in range(1, num+1):
        try:
            # ë§í¬
            p_0 = driver.find_element(By.XPATH, f'//*[@id="search_tabpanel_position"]/div/div[4]/div[{i}]/a').get_attribute('href')
            data_wanted['Lin'].append(p_0)                       

            # íƒ€ì´í‹€
            p_1 = driver.find_element(By.XPATH, f'//*[@id="search_tabpanel_position"]/div/div[4]/div[{i}]/a/div[2]/strong').text
            data_wanted['Tit'].append(p_1)

            # íšŒì‚¬
            p_2 = driver.find_element(By.XPATH, f'//*[@id="search_tabpanel_position"]/div/div[4]/div[{i}]/a/div[2]/span[1]/span[1]').text
            data_wanted['Com'].append(p_2)                       

            # ìœ„ì¹˜
            p_3 = driver.find_element(By.XPATH, f'//*[@id="search_tabpanel_position"]/div/div[4]/div[{i}]/a/div[2]/span[1]/span[2]').text
            data_wanted['Loc'].append(p_3)                    
        except Exception as e:
            st.write(e)
            break

    # ë°ì´í„° ì¶œë ¥
    elem_return_0('ë§í¬', data_wanted['Lin'])
    elem_return_0('íƒ€ì´í‹€', data_wanted['Tit'])
    elem_return_0('íšŒì‚¬', data_wanted['Com'])
    elem_return_0('ìœ„ì¹˜', data_wanted['Loc'])

    for i in data_wanted['Lin']:
        driver.get(i)
        driver.implicitly_wait(2)
        scroll_one(driver)

        # ê³µê³ ë‚´ìš©
        try:
            p_4 = driver.find_element(By.CLASS_NAME, 'JobDescription_JobDescription__VWfcb').text
            data_wanted['Ctn'].append(p_4)
        except Exception as e:
            st.write(e)
            break

    driver.close()

    # ë°ì´í„° ì¶œë ¥
    elem_return_1('ê³µê³ ë‚´ìš©', data_wanted['Ctn'])

    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df_wanted = pd.DataFrame({
        'Title' : data_wanted['Tit'],
        'Company' : data_wanted['Com'],
        'Content' : data_wanted['Ctn'],   
        'Link' : data_wanted['Lin'],
        'Location' : data_wanted['Loc'],
        'label' : f'{KEYWORD}'         
    })
<<<<<<< HEAD
=======

>>>>>>> ce67bb00ce720aa80ebf23f919faf98329d2fd60
    keyword_csv_file = f'{path}/{KEYWORD}_wanted.csv'
    df_wanted.to_csv(keyword_csv_file, index=False, encoding='utf-8-sig')
    df_wanted = pd.read_csv(keyword_csv_file)
    
    return data_wanted
<<<<<<< HEAD
=======

    keyword_csv_file = f'{path}/{KEYWORD}.csv'
    df.to_csv(keyword_csv_file, index=False, encoding='utf-8-sig')

    return data

>>>>>>> ce67bb00ce720aa80ebf23f919faf98329d2fd60

## ì‚¬ëŒì¸, ëŒ€ê¸°ì—… ìˆ˜ì§‘
def Saramin_logic(front_url, mid_url, back_url, page, data):

    # webdriver ì‹¤í–‰
    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))
    driver.get(front_url+KEYWORD+mid_url+str(page)+back_url)
    driver.implicitly_wait(2)
    
    # keyword ì±„ìš©ê³µê³  í™•ì¸ í›„ ì „ì²´ í˜ì´ì§€ ìˆ˜
    ct_result = driver.find_element(By.XPATH, '//*[@id="recruit_info"]/div[1]/span').text
    total_result = re.sub(r'[^0-9]', '', ct_result)
    page_ct = int(int(total_result)/100)

    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶œë ¥
    print(f'\n\n##### {KEYWORD} #####')

    # ê³µê³  ê°œìˆ˜
    for pages in range(1, page_ct+2):
        try:
            driver.get(front_url+KEYWORD+mid_url+str(pages)+back_url)
            driver.implicitly_wait(2)

            div_1 = driver.find_elements(By.CLASS_NAME, "job_tit")
            for item in div_1:
                
                #ë§í¬
                p_0 = item.find_element(By.TAG_NAME, "a").get_attribute('href')
                data['Lin'].append(p_0)

                #íƒ€ì´í‹€
                p_1 = item.find_element(By.TAG_NAME, "a").text
                data['Tit'].append(p_1)

            # íšŒì‚¬
            div_2 = driver.find_elements(By.CLASS_NAME, "item_recruit")
            for item in div_2:
                p_2 = item.find_element(By.TAG_NAME, "a").text
                data['Com'].append(p_2)
            
            #ìœ„ì¹˜
            div_3 = driver.find_elements(By.CLASS_NAME, "job_condition")
            for item in div_3:
                tmp_loc = item.find_element(By.TAG_NAME, "a").text
                p_3 = tmp_loc[:2]
                data['Loc'].append(p_3)
             
            # ë°ì´í„° ì¶œë ¥
            elem_return_0('ë§í¬', data['Lin'])
            elem_return_0('íƒ€ì´í‹€', data['Tit'])
            elem_return_0('íšŒì‚¬', data['Com'])
            elem_return_0('ìœ„ì¹˜', data['Loc'])

        except Exception as e:
            st.write(e)
            break

    return data

## ì‚¬ëŒì¸ ìˆ˜ì§‘
def Saramin(KEYWORD, data_saramin):
    
    page = 1

    front_url = 'https://www.saramin.co.kr/zf_user/search?search_area=main&search_done=y&search_optional_item=n&searchType=search&searchword='
    mid_url = '&recruitPage='
    back_url = '&recruitSort=relation&recruitPageCount=100&inner_com_type=&company_cd=0%2C1%2C2%2C3%2C4%2C5%2C6%2C7&show_applied=&quick_apply=n&except_read=n&ai_head_hunting=n&mainSearch=y'

    data_saramin = Saramin_logic(front_url, mid_url, back_url, page, data_saramin)

    # webdriver ì‹¤í–‰
    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

    for i in data_saramin['Lin']:
        driver.get(i)
        driver.implicitly_wait(2)
        scroll_one(driver)

        # ê³µê³ ë‚´ìš©
        try:
            driver.switch_to.frame("iframe_content_0")
            p_4 = driver.find_element(By.CLASS_NAME, "user_content").text
            data_saramin['Ctn'].append(p_4)

        except Exception as e:
            st.write(e)
            break

    driver.close()

    # ë°ì´í„° ì¶œë ¥
    elem_return_1('ê³µê³ ë‚´ìš©', data_saramin['Ctn'])

        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df_saramin = pd.DataFrame({
        'Title' : data_saramin['Tit'],
        'Company' : data_saramin['Com'],
        'Content' : data_saramin['Ctn'],   
        'Link' : data_saramin['Lin'],
        'Location' : data_saramin['Loc'],
        'label' : f'{KEYWORD}'         
    })

    keyword_csv_file = f'{path}/{KEYWORD}_saramin.csv'
    df_saramin.to_csv(keyword_csv_file, index=False, encoding='utf-8-sig')
    df_saramin = pd.read_csv(keyword_csv_file)

    return data_saramin

## ëŒ€ê¸°ì—… ìˆ˜ì§‘
def Major(KEYWORD, data_major):

    page = 1

    front_url = 'https://www.saramin.co.kr/zf_user/search?search_area=main&search_done=y&search_optional_item=n&searchType=search&searchword='
    mid_url = '&recruitPage='
    back_url = '&recruitSort=relation&recruitPageCount=100&inner_com_type=&company_cd=0%2C1%2C2%2C3%2C4%2C5%2C6%2C7&company_type=scale001&show_applied=&quick_apply=n&except_read=n&ai_head_hunting=n&mainSearch=y'

    data_major = Saramin_logic(front_url, mid_url, back_url, page, data_major)

    # webdriver ì‹¤í–‰
    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

    for i in data_major['Lin']:
        driver.get(i)
        driver.implicitly_wait(2)
        scroll_one(driver)

        # ê³µê³ ë‚´ìš©
        try:
            driver.switch_to.frame("iframe_content_0")
            p_4 = driver.find_element(By.CLASS_NAME, "user_content").text
            data_major['Ctn'].append(p_4)

        except Exception as e:
            st.write(e)
            break

    driver.close()

    # ë°ì´í„° ì¶œë ¥
    elem_return_1('ê³µê³ ë‚´ìš©', data_major['Ctn'])

        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df_major = pd.DataFrame({
        'Title' : data_major['Tit'],
        'Company' : data_major['Com'],
        'Content' : data_major['Ctn'],   
        'Link' : data_major['Lin'],
        'Location' : data_major['Loc'],
        'label' : f'{KEYWORD}'         
    })

    keyword_csv_file = f'{path}/{KEYWORD}_major.csv'
    df_major.to_csv(keyword_csv_file, index=False, encoding='utf-8-sig')
    df_major = pd.read_csv(keyword_csv_file)

    return data_major

## ì±„ìš©ê³µê³  ì•„ì›ƒí’‹
def posting_output(df, posting):
    IDX = df.loc[df.label == f'{posting}'].index.tolist()
    st.write(f'#### {posting}')
    st.write('íŒŒë€ìƒ‰ ê¸€ì”¨ë¥¼ ëˆ„ë¥´ë©´ í•´ë‹¹ ì±„ìš©ê³µê³ ë¡œ ì´ë™ë©ë‹ˆë‹¤.')
    for i in IDX:
        st.write(f'{i}. {com_list[i]}, [{tit_list[i]}]({lin_list[i]})')


## st.button()
KEYWORDS = ['ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸']

# ë¹ˆ íŒŒì¼ ìƒì„±
now_csv_file = f'{path}/{now_name}.csv'
now_csv_file_major = f'{path}/{now_name}_major.csv'

# ë²„íŠ¼ í´ë¦­
if st.button('í¬ë¡¤ë§ ì‹¤í–‰'):
    if not os.path.exists(now_csv_file):
        data_wanted, data_saramin, data_major = make_dict()
        # í¬ë¡¤ë§ ì‹œì‘ 
        st_info = st.info('í¬ë¡¤ë§ ì§„í–‰ ì¤‘')
        for KEYWORD in KEYWORDS:
            data_wanted = Wanted(KEYWORD, data_wanted)
            data_saramin = Saramin(KEYWORD, data_saramin)
            data_major = Major(KEYWORD, data_major)
        

        time.sleep(1)
        st_info.empty()
        st_success = st.success('í¬ë¡¤ë§ ì™„ë£Œ')
        time.sleep(1)
        st_success.empty()

        # Data Preprocessing
        # merge
        st_info = st.info('ë°ì´í„° ì²˜ë¦¬ ì¤‘')

        DA_major = pd.read_csv(f'{path}/{KEYWORDS[0]}_major.csv')
        DS_major = pd.read_csv(f'{path}/{KEYWORDS[1]}_major.csv')
        df = pd.concat([DA_major, DS_major], axis=0)
        df.to_csv(now_csv_file_major, index=False, encoding='utf-8-sig')
        df = pd.read_csv(now_csv_file_major)

        DA_wanted = pd.read_csv(f'{path}/{KEYWORDS[0]}_wanted.csv')
        DS_wanted = pd.read_csv(f'{path}/{KEYWORDS[1]}_wanted.csv')
        DA_saramin = pd.read_csv(f'{path}/{KEYWORDS[0]}_saramin.csv')
        DS_saramin = pd.read_csv(f'{path}/{KEYWORDS[1]}_saramin.csv')
        df = pd.concat([DA_wanted, DS_wanted, DA_saramin, DS_saramin], axis=0)
        df.to_csv(now_csv_file, index=False, encoding='utf-8-sig')
        df = pd.read_csv(now_csv_file)

        # Location
        for i in range(0, len(df)):
            s_0 = df.Location[i].split(' Â· ')
            df.Location[i] = s_0[0]
        df.to_csv(now_csv_file, index=False, encoding='utf-8-sig')

        time.sleep(1)
        st_info.empty()
        st_success = st.success('ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ')
        time.sleep(1) 
        st_success.empty()

        # file remove
        os.remove(f'{path}/{KEYWORDS[0]}_major.csv')
        os.remove(f'{path}/{KEYWORDS[1]}_major.csv')
        os.remove(f'{path}/{KEYWORDS[0]}_wanted.csv')
        os.remove(f'{path}/{KEYWORDS[1]}_wanted.csv')
        os.remove(f'{path}/{KEYWORDS[0]}_saramin.csv')
        os.remove(f'{path}/{KEYWORDS[1]}_saramin.csv')
    else:
        st_info = st.info('ìƒì„±ëœ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤.')
        time.sleep(1)
        st_info.empty()
st.write('')
st.write('')

# st.selectbox()
posting = st.selectbox(
    'ì§ë¬´',
    ('ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸'),
    index=None,
    placeholder='ì§ë¬´ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.'
)
st.write('')
=======
## ë°ì´í„°í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
>>>>>>> upstream/main
try:
    df = pd.read_csv(f'{path}/{now_name}_wanted.csv')
except FileNotFoundError:
    st.error('ìƒì„±ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Intro í˜ì´ì§€ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.')



###########################################################################################



## st.selectbox()

# ë¹ˆ ê°’
state = {
    'plf': None,
    'lab': None,
    'com': None,
    'tit': None,
    'skl': None,
}

# í”Œë«í¼ ì„ íƒ
plf = st.selectbox(
    'í”Œë«í¼',
    ('ì›í‹°ë“œ', 'ìºì¹˜'),
    index=None,
    placeholder='í”Œë«í¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.'
)
state['plf'] = plf

# ì›í‹°ë“œ ì„ íƒ ì‹œ
if state['plf'] == 'ì›í‹°ë“œ':

    # ë ˆì´ë¸” ì„ íƒ
    lab = st.selectbox(
        'ì§ë¬´',
        ('ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸'),
        index=None,
        placeholder='ì§ë¬´ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.'
    )
    state['lab'] = lab

    # íšŒì‚¬ ì„ íƒ
    try:
        company_list = list(set(df.loc[df.label == state['lab']].Company.tolist()))
        com = st.selectbox(
            'íšŒì‚¬',
            (company_list),
            index=None,
            placeholder='íšŒì‚¬ë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.'
        )
        state['com'] = com
    except NameError:
        st.error('ìƒì„±ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Intro í˜ì´ì§€ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.')

    # íƒ€ì´í‹€ ì„ íƒ
    try:
        title_list = df.loc[(df.label == state['lab']) & (df.Company == state['com'])].Title.tolist()
        tit = st.selectbox(
            'ê³µê³ ëª…',
            (title_list),
            index=None,
            placeholder='ê³µê³ ëª…ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.'
        )
        state['tit'] = tit
    except NameError:
        st.error('ìƒì„±ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Intro í˜ì´ì§€ì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.')

    # ì„¸ë¶€ ê³µê³ ë‚´ìš© ì„ íƒ
    skl = st.selectbox(
        'ì„¸ë¶€ ê³µê³ ë‚´ìš©',
        ('ì£¼ìš”ì—…ë¬´', 'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­'),
        index=None,
        placeholder='ë³´ê³ ì‹¶ì€ ì„¸ë¶€ ê³µê³ ë‚´ìš©ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.'
    )
    state['skl'] = skl
    st.write('')

    # st.button()
    if st.button('í™•ì¸'):
        cond = df.loc[(df.label == state['lab']) & (df.Company == state['com']) & (df.Title == state['tit'])]
        # ì£¼ìš”ì—…ë¬´ ì„ íƒ ì‹œ
        if state['skl'] == 'ì£¼ìš”ì—…ë¬´':
            st.write('')
            st.text(cond.Content_0.tolist()[0])
            st.write('')
            link_click = cond.Link.tolist()[0]
            st.write(f'ì±„ìš©ì •ë³´ê°€ ë” ê¶ê¸ˆí•˜ë‹¤ë©´ ë§í¬ í´ë¦­! ğŸ‘‰ [{link_click}]({link_click})')

        # ìê²©ìš”ê±´ ì„ íƒ ì‹œ
        if state['skl'] == 'ìê²©ìš”ê±´':
            st.write('')
            st.text(cond.Content_1.tolist()[0])
            st.write('')
            link_click = cond.Link.tolist()
            st.write(f'ì±„ìš©ì •ë³´ê°€ ë” ê¶ê¸ˆí•˜ë‹¤ë©´ ë§í¬ í´ë¦­! ğŸ‘‰ [{link_click}]({link_click})')

        # ìš°ëŒ€ì‚¬í•­ ì„ íƒ ì‹œ
        if state['skl'] == 'ìš°ëŒ€ì‚¬í•­':
            st.write('')
            st.text(cond.Content_2.tolist()[0])
            st.write('')
            link_click = cond.Link.tolist()
            st.write(f'ì±„ìš©ì •ë³´ê°€ ë” ê¶ê¸ˆí•˜ë‹¤ë©´ ë§í¬ í´ë¦­! ğŸ‘‰ [{link_click}]({link_click})')

# ìºì¹˜ ì„ íƒ ì‹œ
if state['plf'] == 'ìºì¹˜':
    pass